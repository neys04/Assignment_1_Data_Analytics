{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ebf916-f879-4e71-8b20-1d2a592c7156",
   "metadata": {},
   "source": [
    "## **A Python Implementation of Balanced Risk Set Matching**  \n",
    "### *By Jyreneah Angel and Nicole Grace Joligon*  \n",
    "---\n",
    "### **Abstract**\n",
    "This programming assignment focuses on the implementation of Balanced Risk Set Matching (BRSM), a statistical technique used in observational studies to control for confounding variables by matching treated and control individuals with similar baseline characteristics. The assignment is part of a Data Analytics course designed to test algorithmic thinking and practical coding skills.\n",
    "\n",
    "In this assignment, we are tasked with reading the journal article provided and developing a Python implementation of the BRSM methodology. The study referenced in the article explores treatment outcomes for interstitial cystitis, using cystoscopy and hydrodistention, comparing patients with similar symptoms but different timelines for receiving treatment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b8e15f-b23d-4fe6-8a8d-ab495810b716",
   "metadata": {},
   "source": [
    "### **Introduction**\n",
    "\n",
    "The study *\"Balanced Risk Set Matching\"* by *Yunfei Paul Li, Kathleen J. Propert, and Paul R. Rosenbaum* introduces a method called Balanced Risk Set Matching (BRSM) to improve the reliability of observational studies where randomized trials are not feasible. Specifically, the method is used to compare treatments for interstitial cystitis (IC), a chronic bladder condition.\n",
    "\n",
    "BRSM works by matching treated patients with untreated ones who have similar symptom histories, ensuring that comparisons are not biased by future data. Furthermore, the study employs advanced mathematical techniques, such as integer programming, to ensure that matched groups are balanced in key symptoms like pain and urgency.\n",
    "\n",
    "Statistical tests were then used to assess the treatment’s effect. While the results showed small improvements in some symptoms (like nocturnal voiding frequency), there were no strong improvements in others. In addition, sensitivity analysis indicated that hidden biases could influence the results, making the conclusions about treatment efficacy tentative.\n",
    "\n",
    "In light of these findings, a Python implementation of the BRSM methodology would enhance the accessibility, efficiency, and reproducibility of observational studies. Python’s widely used libraries for data analysis, optimization, and statistical testing make it a suitable platform for automating the matching process and ensuring transparent analysis. Moreover, this implementation would enable the handling of large datasets, scale to larger patient populations, and integrate with other analytical tools for further analysis. Additionally, it would facilitate sensitivity analysis to address hidden biases, thereby allowing for broader application of the method across various medical conditions and improving the reliability of treatment effect comparisons in the absence of randomized trials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d83921-5a2d-4a5d-b74f-cfdc0edae78d",
   "metadata": {},
   "source": [
    "### **Step-by-Step Implementation of BRSM Methodology**\n",
    "\n",
    "#### 1. Data Preparation\n",
    "Before performing the analysis, it's essential to ensure that the data is properly prepared and cleaned. This step involves collecting relevant variables, ensuring that both treated and control groups are represented correctly, and preprocessing the data to account for different units and ranges.\n",
    "##### **Objective**:\n",
    "Prepare the data for analysis by ensuring all relevant variables are available and preprocessed.\n",
    "\n",
    "##### **Data Requirements**:\n",
    "- **Treated Group**: Patients who received treatment (e.g., cystoscopy and hydrodistention).\n",
    "- **Control Group**: Patients who did not receive treatment.\n",
    "- **Covariates**: Pre-treatment symptom data, such as pain, urgency, and nocturnal voiding frequency.\n",
    "- **Time Points**: The treatment date for the treated group, with symptom data up until treatment time.\n",
    "\n",
    "##### **Preprocessing**:\n",
    "- Normalize or scale symptom data to account for different ranges and units.\n",
    "\n",
    "#### 2. Risk Set Matching\n",
    "In this section, we will perform the **Risk Set Matching** process. The goal is to pair treated patients with control patients who have similar pre-treatment symptom histories.\n",
    "\n",
    "##### **Objective**:\n",
    "Pair treated patients with control patients who have similar pre-treatment symptoms.\n",
    "\n",
    "##### **Method**:\n",
    "\n",
    "*Distance Measure*:\n",
    "   - We will calculate the **Euclidean distance** (or **Mahalanobis distance**) between the treated and control patients' symptom histories. \n",
    "   - The closest match based on the smallest distance will be selected.\n",
    "     \n",
    "*Matching Process*:\n",
    "   - For each treated patient, identify an untreated patient who has a similar symptom history up to the point of treatment.\n",
    "   - The matching ensures that treatment decisions are based only on pre-treatment data, avoiding the introduction of post-treatment bias in the analysis.\n",
    "     \n",
    "#### 3. Balanced Matching (Integer Programming)\n",
    "\n",
    "In this section, we will perform **Balanced Matching** to ensure that the treatment and control groups are as balanced as possible in terms of pre-treatment symptom distributions.\n",
    "\n",
    "##### **Objective**:\n",
    "Minimize the imbalance in symptom distributions between treated and control groups.\n",
    "\n",
    "##### **Method**:\n",
    "\n",
    "*Optimization*:\n",
    "   - The goal is to minimize the **multivariate distance** between matched pairs while ensuring balance in key symptoms such as **pain**, **urgency**, and **nocturnal voiding frequency**.\n",
    "     \n",
    "*Penalty Function*:\n",
    "   - A **penalty function** is introduced in the optimization process to prioritize balance in these key symptoms. The penalty function ensures that the matchings do not just minimize the distance between symptom histories, but also enforce balance across multiple symptom variables.\n",
    "     \n",
    "*Implementation*:\n",
    "   - We will use **network flow** or other optimization algorithms (e.g., **integer programming**) to ensure that the best possible matches are made while balancing the symptoms. These algorithms will adjust the matches to minimize the total distance while maintaining symmetry across symptom distributions.\n",
    "\n",
    "#### 4. Statistical Analysis\n",
    "\n",
    "In this section, we will perform **Statistical Analysis** to evaluate whether the treatment had a significant impact on the symptoms of patients post-treatment.\n",
    "\n",
    "##### **Objective**:\n",
    "Test if the treatment effect significantly improved symptoms post-treatment.\n",
    "\n",
    "##### **Methods**:\n",
    "\n",
    "*Wilcoxon Signed-Rank Test*:\n",
    "   - The **Wilcoxon Signed-Rank Test** is used to assess whether there is a significant difference between the pre-treatment and post-treatment data within each group (treated vs. control).\n",
    "   - It is a non-parametric test that compares the median of differences between paired observations, making it suitable for non-normally distributed data.\n",
    "\n",
    "*Sensitivity Analysis*:\n",
    "   - **Sensitivity analysis** will be performed to evaluate the potential influence of unobserved confounders on the treatment effect. This helps determine if hidden biases may affect the results and the validity of the conclusions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc37ef3-7c4e-4150-85a2-f6178acacdf1",
   "metadata": {},
   "source": [
    "### **Application of the Risk Set Matching Methodology**\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b9839e25-7af2-4e0c-a7c6-2ee8c8285a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa0b1e-97b4-4d66-aa6c-a8540b145656",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d4725ce8-a7b4-4346-bc2e-6aaed1cf2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_treated = 50\n",
    "n_controls = 150\n",
    "n_patients = n_treated + n_controls\n",
    "\n",
    "# Generate symptom data\n",
    "base_pain = np.random.normal(loc=5, scale=2, size=n_patients).clip(0, 9)\n",
    "base_urgency = np.random.normal(loc=5, scale=1.5, size=n_patients).clip(0, 9)\n",
    "base_frequency = np.random.normal(loc=3, scale=1, size=n_patients).clip(0, 9)\n",
    "\n",
    "# Create treated and control groups\n",
    "treated = pd.DataFrame({\n",
    "    'id': np.arange(n_treated),\n",
    "    'is_treated': True,\n",
    "    'baseline_pain': base_pain[:n_treated],\n",
    "    'baseline_urgency': base_urgency[:n_treated],\n",
    "    'baseline_frequency': base_frequency[:n_treated],\n",
    "    'pain_at_t': (base_pain[:n_treated] + np.random.normal(1, 0.5, n_treated)).clip(0, 9),\n",
    "    'urgency_at_t': (base_urgency[:n_treated] + np.random.normal(1, 0.5, n_treated)).clip(0, 9),\n",
    "    'frequency_at_t': (base_frequency[:n_treated] + np.random.normal(1, 0.5, n_treated)).clip(0, 9),\n",
    "})\n",
    "\n",
    "controls = pd.DataFrame({\n",
    "    'id': np.arange(n_treated, n_patients),\n",
    "    'is_treated': False,\n",
    "    'baseline_pain': base_pain[n_treated:],\n",
    "    'baseline_urgency': base_urgency[n_treated:],\n",
    "    'baseline_frequency': base_frequency[n_treated:],\n",
    "    'pain_at_t': (base_pain[n_treated:] + np.random.normal(0, 0.3, n_controls)).clip(0, 9),\n",
    "    'urgency_at_t': (base_urgency[n_treated:] + np.random.normal(0, 0.3, n_controls)).clip(0, 9),\n",
    "    'frequency_at_t': (base_frequency[n_treated:] + np.random.normal(0, 0.3, n_controls)).clip(0, 9),\n",
    "})\n",
    "\n",
    "# Simulate post-treatment outcomes (3 months and 6 months)\n",
    "def simulate_outcomes(base_value, treatment_effect, decay_rate):\n",
    "    return (\n",
    "        np.clip(base_value - treatment_effect + np.random.normal(0, 1, len(base_value)), 0, 9),\n",
    "        np.clip(base_value - (treatment_effect * decay_rate) + np.random.normal(0, 1, len(base_value)), 0, 9)\n",
    "    )\n",
    "\n",
    "treated['pain_3mo'], treated['pain_6mo'] = simulate_outcomes(treated['pain_at_t'], 1, 0.8)\n",
    "treated['urgency_3mo'], treated['urgency_6mo'] = simulate_outcomes(treated['urgency_at_t'], 1, 0.8)\n",
    "treated['frequency_3mo'], treated['frequency_6mo'] = simulate_outcomes(treated['frequency_at_t'], 1, 0.8)\n",
    "\n",
    "controls['pain_3mo'] = np.clip(controls['pain_at_t'] + np.random.normal(0, 0.5, len(controls)), 0, 9)\n",
    "controls['pain_6mo'] = np.clip(controls['pain_3mo'] + np.random.normal(0, 0.5, len(controls)), 0, 9)\n",
    "controls['urgency_3mo'] = np.clip(controls['urgency_at_t'] + np.random.normal(0, 0.5, len(controls)), 0, 9)\n",
    "controls['urgency_6mo'] = np.clip(controls['urgency_3mo'] + np.random.normal(0, 0.5, len(controls)), 0, 9)\n",
    "controls['frequency_3mo'] = np.clip(controls['frequency_at_t'] + np.random.normal(0, 0.5, len(controls)), 0, 9)\n",
    "controls['frequency_6mo'] = np.clip(controls['frequency_3mo'] + np.random.normal(0, 0.5, len(controls)), 0, 9)\n",
    "\n",
    "all_data = pd.concat([treated, controls], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26031e9-a34d-4ca9-b045-2ed879d04519",
   "metadata": {},
   "source": [
    "#### Risk Set Matching - *Distance Measure*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5a4fab67-7e9a-4ed4-b0ec-e2fad584ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mahalanobis(treated_df, control_df):\n",
    "    features = ['baseline_pain', 'baseline_urgency', 'baseline_frequency']\n",
    "    scaler = StandardScaler()\n",
    "    combined_data = scaler.fit_transform(pd.concat([treated_df[features], control_df[features]]))\n",
    "    \n",
    "    treated_scaled = combined_data[:len(treated_df)]\n",
    "    control_scaled = combined_data[len(treated_df):]\n",
    "    inv_cov_matrix = np.linalg.pinv(np.cov(combined_data.T))\n",
    "    \n",
    "    distances = cdist(treated_scaled, control_scaled, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "    return distances\n",
    "\n",
    "distance_matrix = compute_mahalanobis(treated, controls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae15a19-2f39-4bf6-8a4c-6d03bac363a4",
   "metadata": {},
   "source": [
    "#### Risk Set Matching - *Matching Process*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "52394bcf-30db-48d4-93bc-bf2df60b82f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_patients(distance_matrix):\n",
    "    row_ind, col_ind = linear_sum_assignment(distance_matrix)\n",
    "    matches = list(zip(row_ind, col_ind))\n",
    "    return matches\n",
    "\n",
    "matches = match_patients(distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0fdf9-323a-43de-9fae-d6e452f6de30",
   "metadata": {},
   "source": [
    "#### Balanced Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "989e963d-c6a2-4510-a7cc-f3162e6483a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_patients(distance_matrix):\n",
    "    row_ind, col_ind = linear_sum_assignment(distance_matrix)\n",
    "    matches = list(zip(row_ind, col_ind))\n",
    "    return matches\n",
    "\n",
    "matches = match_patients(distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e124a-d3a1-4c24-9607-cd2b4d42a4ba",
   "metadata": {},
   "source": [
    "#### Statistical Analysis - *Wilcoxon Signed-Rank Test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00889c60-b127-4695-9f0e-87319cc34020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_wilcoxon_test(df, matches):\n",
    "    treated_outcomes = [df.loc[df['id'] == treated, 'pain_at_t'].values[0] for treated, control in matches]\n",
    "    control_outcomes = [df.loc[df['id'] == control, 'pain_at_t'].values[0] for treated, control in matches]\n",
    "    \n",
    "    if np.all(np.array(treated_outcomes) == np.array(control_outcomes)):\n",
    "        return 0, 1.0  # No difference, p-value is 1\n",
    "    else:\n",
    "        stat, p_value = wilcoxon(treated_outcomes, control_outcomes)\n",
    "        return stat, p_value\n",
    "\n",
    "wilcoxon_stat, p_value = perform_wilcoxon_test(all_data, matches)\n",
    "print(f\"Wilcoxon test statistic: {wilcoxon_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b0df1-8a62-4076-a72d-ae4fe7a42ea6",
   "metadata": {},
   "source": [
    "#### Statistical Analysis - *Sensitivity Analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4b9aa5a2-6cba-4402-818c-64b39a3918b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity Analysis Results:\n",
      "Gamma: 0.00, P-value: 0.4548\n",
      "Gamma: 0.11, P-value: 0.0181\n",
      "Gamma: 0.22, P-value: 0.0001\n",
      "Gamma: 0.33, P-value: 0.0000\n",
      "Gamma: 0.44, P-value: 0.0000\n",
      "Gamma: 0.56, P-value: 0.0000\n",
      "Gamma: 0.67, P-value: 0.0000\n",
      "Gamma: 0.78, P-value: 0.0000\n",
      "Gamma: 0.89, P-value: 0.0000\n",
      "Gamma: 1.00, P-value: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def sensitivity_analysis(df, matches, gamma_range):\n",
    "    results = []\n",
    "    for gamma in gamma_range:\n",
    "        adjusted_treated = [\n",
    "            df.loc[df['id'] == treated, 'pain_at_t'].values[0] * (1 + gamma)\n",
    "            for treated, control in matches\n",
    "        ]\n",
    "        control_outcomes = [df.loc[df['id'] == control, 'pain_at_t'].values[0] for treated, control in matches]\n",
    "        stat, p_value = wilcoxon(adjusted_treated, control_outcomes)\n",
    "        results.append((gamma, p_value))\n",
    "    return results\n",
    "\n",
    "gamma_range = np.linspace(0, 1, 10)\n",
    "sensitivity_results = sensitivity_analysis(all_data, matches, gamma_range)\n",
    "print(\"Sensitivity Analysis Results:\")\n",
    "for gamma, p_val in sensitivity_results:\n",
    "    print(f\"Gamma: {gamma:.2f}, P-value: {p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b82b22-c26c-4806-8818-5a9d056138cf",
   "metadata": {},
   "source": [
    "### **Visualization of Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e290c-df1f-415f-a020-295ebeb34fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
